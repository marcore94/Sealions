{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\vincenzo\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import cv2\n",
    "from skimage.transform import rotate, resize, SimilarityTransform, warp\n",
    "import os\n",
    "import numpy as np\n",
    "import random as rand\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.layers import Flatten, Dense\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SHAPE = (96, 96, 3)\n",
    "IM_HEIGHT = 96\n",
    "IM_WIDTH = 96\n",
    "OUTPUT_SIZE = 2\n",
    "\n",
    "LEARNING_RATE = 0.0005\n",
    "OPTIMIZER = keras.optimizers.Adam(lr=LEARNING_RATE)\n",
    "LOSS = 'binary_crossentropy'\n",
    "METRIC = 'accuracy'\n",
    "\n",
    "SL_TRAIN_SIZE = 40411\n",
    "SL_VALIDATION_SIZE = 9668\n",
    "SL_TEST_SIZE = 13539\n",
    "EPOCHS = 5\n",
    "VALIDATION_SPLIT = 0.2\n",
    "BATCH_SIZE = 50\n",
    "STEPS_PER_EPOCH = (2 * SL_TRAIN_SIZE) // BATCH_SIZE + 1\n",
    "VALIDATION_STEPS_PER_EPOCH = (2 * SL_VALIDATION_SIZE) // BATCH_SIZE + 1\n",
    "MAX_EPOCHS_WITH_SAME_DATA_SET = 20\n",
    "\n",
    "TRAIN_PATH = \"./data_set/train/\"\n",
    "VALIDATION_PATH = \"./data_set/validation/\"\n",
    "TEST_PATH = \"./data_set/test/\"\n",
    "\n",
    "MODEL_PATH = \"./binary_classifier/net_2_model.h5\"\n",
    "\n",
    "MR_CKPT_PATH = \"./binary_classifier/net_2_most_recent_checkpoint.hdf5\"\n",
    "CB_CKPT_PATH = \"./binary_classifier/net_2_current_best_checkpoint.hdf5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_image(img):\n",
    "    sel = [0, 1]\n",
    "    offset = 72\n",
    "    bound = (72 - 48) / 2\n",
    "    t_img = np.array(img)\n",
    "    \n",
    "    # Rotate\n",
    "    angle = rand.uniform(0, 360)\n",
    "    t_img = rotate(t_img, angle, mode='edge')\n",
    "    \n",
    "    # Flip\n",
    "    if rand.choice(sel):\n",
    "        t_img = np.flipud(t_img)\n",
    "    if rand.choice(sel):\n",
    "        t_img = np.fliplr(t_img)\n",
    "    \n",
    "    # Shift\n",
    "    tform = SimilarityTransform(translation=(int(rand.uniform(-bound, bound)), int(rand.uniform(-bound, bound))))\n",
    "    t_img = warp(t_img, tform)\n",
    "    \n",
    "    # Zoom and cut 96x96 patch\n",
    "    zoom = int(rand.uniform(-bound, bound))\n",
    "    t_img = resize(t_img[offset-48-zoom:offset+48+zoom, offset-48-zoom:offset+48+zoom, :], (96, 96))\n",
    "    return t_img.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_generator():\n",
    "    sl_lst_tmp = os.listdir(TRAIN_PATH + '0_sea_lions')\n",
    "    sl_lst = []\n",
    "    for elem in sl_lst_tmp:\n",
    "        sl_lst.append(list((cv2.imread(TRAIN_PATH + '0_sea_lions/' + elem), 'sea_lion')))\n",
    "    bkg_lst_tmp = os.listdir(TRAIN_PATH + '1_background')\n",
    "    while True:\n",
    "        img_lst = []\n",
    "        for elem in rand.sample(bkg_lst_tmp, SL_TRAIN_SIZE):\n",
    "            img_lst.append(list((cv2.imread(TRAIN_PATH + '1_background/' + elem), 'background')))\n",
    "        img_lst = sl_lst + img_lst\n",
    "        rand.shuffle(img_lst)\n",
    "        for i in range(MAX_EPOCHS_WITH_SAME_DATA_SET):\n",
    "            patches = []\n",
    "            classes = []\n",
    "            curr_batch_size = 0\n",
    "            for elem in img_lst:\n",
    "                if elem[1] == 'background':\n",
    "                    patches.append(elem[0])\n",
    "                    classes.append([0, 1])\n",
    "                else:\n",
    "                    patches.append(transform_image(elem[0]))\n",
    "                    classes.append([1, 0])\n",
    "                curr_batch_size += 1\n",
    "                if curr_batch_size == BATCH_SIZE:\n",
    "                    X_train = np.array(patches)\n",
    "                    X_train = X_train.astype('float32')\n",
    "                    X_train /= 255\n",
    "                    Y_train = np.array(classes)\n",
    "                    curr_batch_size = 0\n",
    "                    patches = []\n",
    "                    classes = []\n",
    "                    yield X_train, Y_train\n",
    "            if len(patches) > 0:\n",
    "                X_train = np.array(patches)\n",
    "                X_train = X_train.astype('float32')\n",
    "                X_train /= 255\n",
    "                Y_train = np.array(classes)\n",
    "                yield X_train, Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation data set\n",
    "X_validation = []\n",
    "Y_validation = []\n",
    "validation_set = []\n",
    "lst = os.listdir(VALIDATION_PATH + '0_sea_lions')\n",
    "for elem in lst:\n",
    "    validation_set.append(list((cv2.imread(VALIDATION_PATH + '0_sea_lions/' + elem), 'sea_lion')))\n",
    "lst = os.listdir(VALIDATION_PATH + '1_background')\n",
    "for elem in lst:\n",
    "    validation_set.append(list((cv2.imread(VALIDATION_PATH + '1_background/' + elem), 'background')))\n",
    "rand.shuffle(validation_set)\n",
    "for data in validation_set:\n",
    "    X_validation.append(data[0])\n",
    "    if data[1] == 'sea_lion':\n",
    "        Y_validation.append([1, 0])\n",
    "    else:\n",
    "        Y_validation.append([0, 1])\n",
    "\n",
    "X_validation = np.array(X_validation, copy=False)\n",
    "# Convert data types and normalize values\n",
    "X_validation = X_validation.astype('float32')\n",
    "X_validation /= 255\n",
    "Y_validation = np.array(Y_validation, copy=False)\n",
    "\n",
    "# Free memory\n",
    "lst = []\n",
    "validation_set = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model\n",
    "\n",
    "model = Sequential()\n",
    "# First layer\n",
    "model.add(Convolution2D(8, (5, 5), activation='relu', padding='valid', input_shape=INPUT_SHAPE))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Second layer\n",
    "model.add(Convolution2D(5, (3, 3), activation='relu', padding='valid'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Third layer\n",
    "model.add(Convolution2D(5, (3, 3), activation='relu', padding='valid'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Fourth layer\n",
    "model.add(Convolution2D(10, (3, 3), activation='relu', padding='valid'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(OUTPUT_SIZE, activation='softmax'))\n",
    "\n",
    "model.compile(loss=LOSS, optimizer=OPTIMIZER, metrics=[METRIC])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpointers\n",
    "\n",
    "# most recent checkpoint\n",
    "mr_checkpointer = ModelCheckpoint(filepath=MR_CKPT_PATH, verbose=1, save_best_only=False)\n",
    "# current best checkpoint\n",
    "cb_checkpointer = ModelCheckpoint(filepath=CB_CKPT_PATH, verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "\n",
    "# Fit model on training data\n",
    "history = model.fit_generator(\n",
    "    train_generator(),\n",
    "    steps_per_epoch=STEPS_PER_EPOCH,\n",
    "    epochs=EPOCHS,\n",
    "    verbose=1,\n",
    "    validation_data=(X_validation, Y_validation),\n",
    "    validation_steps=VALIDATION_STEPS_PER_EPOCH,\n",
    "    workers=8,\n",
    "    max_queue_size=50,\n",
    "    callbacks=[mr_checkpointer, cb_checkpointer])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n",
    "# Summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained model\n",
    "\n",
    "# serialize model to HDF5\n",
    "model.save(MODEL_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
